{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b628d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):  # 天然支持任意维度\n",
    "    return 1 / (1 + np.exp(-a))\n",
    "\n",
    "def relu(a):  # 同上，除了 softmax 比较特殊，其他激活函数都天然支持\n",
    "    return np.maximum(a, 0)\n",
    "\n",
    "# def softmax(a):  # 只支持1维\n",
    "#     return np.exp(a) / np.sum(np.exp(a))\n",
    "\n",
    "# def softmax(a):  # 只支持2维\n",
    "#     sum_exp = np.sum(np.exp(a), axis=1, keepdims=True)\n",
    "#     return a / sum_exp\n",
    "\n",
    "def softmax(a):  # 同时兼容1/2维\n",
    "    if a.ndim == 1:\n",
    "        a = a.reshape(1, -1)\n",
    "    \n",
    "    a_max = np.max(a, axis=1, keepdims=True)  # 数值稳定版\n",
    "    a_exp = np.exp(a - a_max)\n",
    "    sum_exp = np.sum(a_exp, axis=1, keepdims=True)\n",
    "    return a_exp / sum_exp\n",
    "\n",
    "def numerical_gradient(f, x, h = 1e-6):\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])  # type: ignore\n",
    "\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "\n",
    "        tmp = x[idx]\n",
    "\n",
    "        x[idx] = tmp + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp - h\n",
    "        fxh2 = f(x)  # 这里必须提前两个都算出来\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "\n",
    "        x[idx] = tmp\n",
    "\n",
    "        it.iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size) -> None:\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):  # 天然支持多维数组\n",
    "        a1 = np.dot(x, self.params['W1']) + self.params['b1']\n",
    "        # z1 = sigmoid(a1)\n",
    "        z1 = relu(a1)\n",
    "        a2 = np.dot(z1, self.params['W2']) + self.params['b2']\n",
    "        y = softmax(a2)\n",
    "        return y\n",
    "    \n",
    "    # def loss(self, x, t):  # 不支持多维数组\n",
    "    #     y = self.predict(x)\n",
    "    #     return -np.log(y[t])  # 交叉损失熵\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "        \n",
    "        y = self.predict(x)\n",
    "        return -np.mean(np.log(y[np.arange(y.shape[0]), t]))\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        return np.sum(y == t) / y.shape[0]\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84545ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='../data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='../data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9850204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取数据至np.array\n",
    "x_train_list, t_train_list = [], []\n",
    "for image, label in train_dataset:\n",
    "    x_train_list.append(image)\n",
    "    t_train_list.append(label)\n",
    "\n",
    "x_train = np.array(x_train_list)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "t_train = np.array(t_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1e17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化网络\n",
    "net = TwoLayerNet(784, 100, 10)\n",
    "\n",
    "y_train = net.predict(x_train)\n",
    "print(y_train.shape)\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(net.accuracy(x_train, t_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单样本训练测试\n",
    "x = x_train[0]\n",
    "t = t_train[0]\n",
    "\n",
    "print(f\"epoch: 0\")\n",
    "y = net.predict(x)\n",
    "print(f\"softmax: {y}\")\n",
    "y = np.argmax(y)\n",
    "print(f\"y: {np.argmax(y)}\")\n",
    "print(f\"loss: {net.loss(x, t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a0a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    grad = net.numerical_gradient(x, t)\n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        net.params[key] -= 0.01 * grad[key]\n",
    "    y = net.predict(x)\n",
    "    print(f\"epoch: {i + 1}\")\n",
    "    print(f\"softmax: {y}\")\n",
    "    print(f\"y: {np.argmax(y)}\")\n",
    "    print(f\"loss: {net.loss(x, t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3dedf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch 训练测试\n",
    "x = x_train[0:10]\n",
    "t = t_train[0:10]\n",
    "print(x.shape)\n",
    "\n",
    "print(f\"epoch: 0\")\n",
    "y = net.predict(x)\n",
    "print(f\"softmax: {y}\")\n",
    "print(f\"y: {np.argmax(y, axis=1)}\")\n",
    "print(f\"t: {t}\")\n",
    "print(f\"loss: {net.loss(x, t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    grad = net.numerical_gradient(x, t)\n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        net.params[key] -= 0.01 * grad[key]\n",
    "    y = net.predict(x)\n",
    "    print(f\"epoch: {i + 1}\")\n",
    "    print(f\"y: {np.argmax(y, axis=1)}\")\n",
    "    print(f\"loss: {net.loss(x, t)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7846ee8",
   "metadata": {},
   "source": [
    "# mini-bacth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TwoLayerNet(784, 100, 10)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# 超参数\n",
    "iters_num = 10\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 获取 mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 计算梯度\n",
    "    grad = net.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    # 更新参数\n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        net.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 记录学习过程\n",
    "    loss = net.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb288ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(train_loss_list)), train_loss_list)\n",
    "plt.axis('on')\n",
    "plt.grid()\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50a098",
   "metadata": {},
   "source": [
    "# epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仅代码示例，实际上CPU根本跑不动\n",
    "net = TwoLayerNet(784, 100, 10)\n",
    "\n",
    "# 超参数\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = train_size / batch_size\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 获取 mini-batch\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 计算梯度\n",
    "    grad = net.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    # 更新参数\n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        net.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 记录学习过程\n",
    "    loss = net.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 计算每个 epoch 的识别精度\n",
    "    if i / iter_per_epoch == 0:  # 这里应该是 i + 1 吧？\n",
    "        train_acc = net.accuracy(x_train, t_train)\n",
    "        # test_acc = net.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        # test_acc_list.append(test_acc)\n",
    "        print(f\"train acc | {train_acc}\")  # 书中的写法字符串拼接过于古老，这里使用f\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
